{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Comparaison des différents modèles d'extraction de features : \n",
    "\n",
    "* VGG16\n",
    "* VGG19\n",
    "* MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Démultiplication images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROTATIONS = 2\n",
    "def demultiplier_image(img, number_of_rotation):\n",
    "    images = []\n",
    "    for i in range(number_of_rotation):\n",
    "        rotation = (360 / number_of_rotation) * i\n",
    "        new_im = img.rotate(rotation)\n",
    "        images.append(new_im)\n",
    "    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    for i in range(number_of_rotation):\n",
    "        rotation = (360 / number_of_rotation) * i\n",
    "        new_im = img.rotate(rotation)\n",
    "        images.append(new_im)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_root = '/home/jpc/Dev/5A/CODEV_D/dataset/'\n",
    "dataset_root = os.path.join(*'../../../ksc-data/images'.split('/'))\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "# Récupération des images et des labels\n",
    "for dirpath, directories, files in os.walk(dataset_root):\n",
    "    for file in files:\n",
    "        if '.jpg' in file or '.png' in file:\n",
    "            img = Image.open(os.path.join(dirpath, file)).resize((224, 224))\n",
    "            for img in demultiplier_image(img, N_ROTATIONS):  \n",
    "                img = np.array(img)\n",
    "                img = img[:,:,:3] # removing the 4th channel (transparency in png) if present\n",
    "                img = img/255.0 # Convert between 0 and 1\n",
    "                x.append(img)\n",
    "                label = os.path.basename(file.split('_')[0])\n",
    "                y.append(label.lower())\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(812, 224, 224, 3)\n",
      "(812,)\n",
      "['ia' 'ib' 'ic' 'id' 'ie' 'iia' 'iib' 'iiia' 'iiib' 'iva1' 'iva2' 'ivb'\n",
      " 'ivc' 'ivd' 'va' 'via']\n"
     ]
    }
   ],
   "source": [
    "# Réduction du nombre de classes\n",
    "# class_to_keep = np.array(['ia', 'iiib', 'iib'])\n",
    "# mask = np.in1d(y, class_to_keep, invert=True)\n",
    "# y[mask] = 'autre '\n",
    "\n",
    "# Transformation des labels en entier\n",
    "labels, y = np.unique(y, return_inverse=True)\n",
    "\n",
    "# Conversion en \"one hot\"\n",
    "#y = np.eye(len(labels))[y]\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('features/y.lab', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(812, 7, 7, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=x[0].shape)\n",
    "features = vgg16.predict(x)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save('features/VGG16.features', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del features, vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 8s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(812, 7, 7, 512)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=x[0].shape)\n",
    "features = vgg19.predict(x)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(812, 25088)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.reshape(features.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('features/VGG19.features', features)\n",
    "del features, vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet101V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(812, 7, 7, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet101V2\n",
    "\n",
    "resnet101 = ResNet101V2(include_top=False, weights='imagenet', input_shape=x[0].shape)\n",
    "features = resnet101.predict(x)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('features/resnet101.features', features)\n",
    "del features, resnet101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet 152V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(812, 7, 7, 2048)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet152V2\n",
    "\n",
    "resnet152 = ResNet152V2(include_top=False, weights='imagenet', input_shape=x[0].shape)\n",
    "features = resnet152.predict(x)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('features/resnet152.features', features)\n",
    "del features, resnet152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('features/VGG19.features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classifier & compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VGG19.features.npy',\n",
       " 'resnet152.features.npy',\n",
       " 'resnet101.features.npy',\n",
       " 'VGG16.features.npy']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "features_path = 'features/'\n",
    "featuresfiles = [f for f in listdir(features_path) if isfile(join(features_path, f)) and 'features' in f]\n",
    "featuresfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: VGG19\n",
      "Features shape: (812, 25088)\n",
      "Seed: 0\n",
      "{'max_depth': 25, 'n_estimators': 50}\n",
      "Best score: 0.5520282186948854\n",
      "Seed: 42\n",
      "{'max_depth': 500, 'n_estimators': 100}\n",
      "Best score: 0.5608465608465608\n",
      "Seed: 420\n",
      "{'max_depth': 25, 'n_estimators': 250}\n",
      "Best score: 0.5749559082892416\n",
      "\n",
      "Model: resnet152\n",
      "Features shape: (812, 100352)\n",
      "Seed: 0\n",
      "{'max_depth': 25, 'n_estimators': 50}\n",
      "Best score: 0.12698412698412698\n",
      "Seed: 42\n",
      "{'max_depth': 500, 'n_estimators': 100}\n",
      "Best score: 0.10405643738977072\n",
      "Seed: 420\n",
      "{'max_depth': 50, 'n_estimators': 50}\n",
      "Best score: 0.15167548500881833\n",
      "\n",
      "Model: resnet101\n",
      "Features shape: (812, 100352)\n",
      "Seed: 0\n",
      "{'max_depth': 1000, 'n_estimators': 50}\n",
      "Best score: 0.09523809523809523\n",
      "Seed: 42\n",
      "{'max_depth': 25, 'n_estimators': 50}\n",
      "Best score: 0.10582010582010581\n",
      "Seed: 420\n",
      "{'max_depth': 500, 'n_estimators': 50}\n",
      "Best score: 0.09876543209876543\n",
      "\n",
      "Model: VGG16\n",
      "Features shape: (812, 25088)\n",
      "Seed: 0\n",
      "{'max_depth': 50, 'n_estimators': 50}\n",
      "Best score: 0.07407407407407407\n",
      "Seed: 42\n",
      "{'max_depth': 100, 'n_estimators': 50}\n",
      "Best score: 0.15696649029982362\n",
      "Seed: 420\n",
      "{'max_depth': 1000, 'n_estimators': 50}\n",
      "Best score: 0.10758377425044091\n"
     ]
    }
   ],
   "source": [
    "# Loading y\n",
    "y = np.load(features_path+'y.lab.npy')\n",
    "\n",
    "for features_file in featuresfiles:\n",
    "    \n",
    "    # Loading features\n",
    "    features = np.load(features_path+features_file)\n",
    "    print('\\nModel:', features_file.split('.')[0])\n",
    "    \n",
    "    # Flattening\n",
    "    new_x = []\n",
    "    for feature in features:\n",
    "        new_x.append(feature.flatten())\n",
    "    x = np.array(new_x)\n",
    "    del new_x\n",
    "    print('Features shape:', x.shape)\n",
    "    \n",
    "    # Making 3 different test and train set (seed fixed so that all models have the same data)\n",
    "    for seed in (0, 42, 420):\n",
    "        print('Seed:', seed)\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Mélange du dataset\n",
    "        shuffled_indices = np.arange(len(x))\n",
    "        np.random.shuffle(shuffled_indices)\n",
    "        x = x[shuffled_indices]\n",
    "        y = y[shuffled_indices]\n",
    "\n",
    "        # Séparation en test et entraînement 30/70\n",
    "        split_indice = int(0.7 * len(x))\n",
    "        x_train = x[:split_indice - 1]\n",
    "        y_train = y[:split_indice - 1]\n",
    "#         x_test = x[split_indice:]\n",
    "#         y_test = y[split_indice:]\n",
    "        \n",
    "        rf = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 250],\n",
    "            'max_depth': [25, 50, 100, 500, 1000]\n",
    "        }\n",
    "        grid = GridSearchCV(rf, param_grid, cv=5)\n",
    "        grid.fit(x_train, y_train)\n",
    "        \n",
    "        print(grid.best_params_)\n",
    "        print('Best score:', grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[216, 32, 8, 32, 24, 16, 112, 24, 128, 32, 12, 32, 32, 40, 40, 32]\n",
      "['ia' 'ib' 'ic' 'id' 'ie' 'iia' 'iib' 'iiia' 'iiib' 'iva1' 'iva2' 'ivb'\n",
      " 'ivc' 'ivd' 'va' 'via']\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def print_distribution(data):\n",
    "    h = [0, 0, 0, 0]\n",
    "    for r in data:\n",
    "        h[np.where(r==1)[0][0]] += 1  \n",
    "    print(h)\n",
    "    \n",
    "def print_distribution2(data):\n",
    "    h = [0]*16\n",
    "    for r in data:\n",
    "        h[r] += 1  \n",
    "    print(h)\n",
    "    \n",
    "print_distribution2(y)\n",
    "print(labels)\n",
    "\n",
    "# [356, 216, 112, 128]\n",
    "# ['autr' 'ia' 'iib' 'iiib']\n",
    "\n",
    "# [216, 32, 8, 32, 24, 16, 112, 24, 128, 32, 12, 32, 32, 40, 40, 32]\n",
    "# ['ia' 'ib' 'ic' 'id' 'ie' 'iia' 'iib' 'iiia' 'iiib' 'iva1' 'iva2' 'ivb'\n",
    "#  'ivc' 'ivd' 'va' 'via']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: VGG19\n",
      "Features shape: (812, 25088)\n",
      "[267, 148, 72, 80]\n",
      "[88, 68, 40, 48]\n",
      "Iteration 1, loss = 9.73747659\n",
      "Iteration 2, loss = 3.18615120\n",
      "Iteration 3, loss = 1.15715582\n",
      "Iteration 4, loss = 0.76910915\n",
      "Iteration 5, loss = 0.65509400\n",
      "Iteration 6, loss = 0.57258854\n",
      "Iteration 7, loss = 0.51183328\n",
      "Iteration 8, loss = 0.34065546\n",
      "Iteration 9, loss = 0.28318255\n",
      "Iteration 10, loss = 0.24019363\n",
      "Iteration 11, loss = 0.21712063\n",
      "Iteration 12, loss = 0.16881441\n",
      "Iteration 13, loss = 0.13216966\n",
      "Iteration 14, loss = 0.11496612\n",
      "Iteration 15, loss = 0.10517785\n",
      "Iteration 16, loss = 0.09048243\n",
      "Iteration 17, loss = 0.07468638\n",
      "Iteration 18, loss = 0.07350383\n",
      "Iteration 19, loss = 0.06797578\n",
      "Iteration 20, loss = 0.05426327\n",
      "Iteration 21, loss = 0.03929118\n",
      "Iteration 22, loss = 0.03053710\n",
      "Iteration 23, loss = 0.02616510\n",
      "Iteration 24, loss = 0.02231976\n",
      "Iteration 25, loss = 0.01841430\n",
      "Iteration 26, loss = 0.01635413\n",
      "Iteration 27, loss = 0.01582439\n",
      "Iteration 28, loss = 0.01273289\n",
      "Iteration 29, loss = 0.01152053\n",
      "Iteration 30, loss = 0.01048023\n",
      "Iteration 31, loss = 0.00951945\n",
      "Iteration 32, loss = 0.00809896\n",
      "Iteration 33, loss = 0.00747676\n",
      "Iteration 34, loss = 0.00681093\n",
      "Iteration 35, loss = 0.00626917\n",
      "Iteration 36, loss = 0.00592441\n",
      "Iteration 37, loss = 0.00554542\n",
      "Iteration 38, loss = 0.00544098\n",
      "Iteration 39, loss = 0.00520692\n",
      "Iteration 40, loss = 0.00504655\n",
      "Iteration 41, loss = 0.00478155\n",
      "Iteration 42, loss = 0.00462073\n",
      "Iteration 43, loss = 0.00451174\n",
      "Iteration 44, loss = 0.00444077\n",
      "Iteration 45, loss = 0.00436271\n",
      "Iteration 46, loss = 0.00430415\n",
      "Iteration 47, loss = 0.00423776\n",
      "Iteration 48, loss = 0.00415178\n",
      "Iteration 49, loss = 0.00410634\n",
      "Iteration 50, loss = 0.00405589\n",
      "Iteration 51, loss = 0.00402302\n",
      "Iteration 52, loss = 0.00398060\n",
      "Iteration 53, loss = 0.00395129\n",
      "Iteration 54, loss = 0.00391476\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP test accuracy: 0.7909836065573771\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "featuresfiles = ['VGG19.features.npy']\n",
    "\n",
    "\n",
    "for features_file in featuresfiles:\n",
    "    \n",
    "    # Loading features\n",
    "    features = np.load(features_path+features_file)\n",
    "    print('\\nModel:', features_file.split('.')[0])\n",
    "    \n",
    "    # Loading y\n",
    "    y = np.load(features_path+'y.lab.npy')\n",
    "    \n",
    "    # Flattening\n",
    "    new_x = []\n",
    "    for feature in features:\n",
    "        new_x.append(feature.flatten())\n",
    "    x = np.array(new_x)\n",
    "    del new_x, features\n",
    "    print('Features shape:', x.shape)\n",
    "\n",
    "    # Séparation en test et entraînement 30/70\n",
    "    split_indice = int(0.7 * len(x))\n",
    "    x_train = x[:split_indice - 1]\n",
    "    y_train = y[:split_indice - 1]\n",
    "    x_test = x[split_indice:]\n",
    "    y_test = y[split_indice:]\n",
    "\n",
    "    print_distribution(y_train)\n",
    "    print_distribution(y_test)\n",
    "    del x, y\n",
    "    model = MLPClassifier(\n",
    "                        (2000, 500, 100),\n",
    "                        batch_size=64,\n",
    "                        verbose = True)\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    print('MLP test accuracy:', model.score(x_test, y_test))\n",
    "\n",
    " \n",
    " # [267, 148, 72, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlp795.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model, 'mlp795.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "model = load('mlp795.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-78be10c07958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvgg19\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=x[0].shape)\n",
    "\n",
    "model.predict(vgg19(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: VGG19\n",
      "Features shape: (812, 7, 7, 512)\n",
      "[267, 148, 72, 80]\n",
      "[88, 68, 40, 48]\n",
      "Train on 567 samples\n",
      "Epoch 1/50\n",
      "567/567 [==============================] - 3s 5ms/sample - loss: 6.4420 - accuracy: 0.5701\n",
      "Epoch 2/50\n",
      "567/567 [==============================] - 3s 5ms/sample - loss: 6.5866 - accuracy: 0.5705\n",
      "Epoch 3/50\n",
      "567/567 [==============================] - 3s 5ms/sample - loss: 6.5866 - accuracy: 0.5705\n",
      "Epoch 4/50\n",
      "567/567 [==============================] - 3s 5ms/sample - loss: 6.5866 - accuracy: 0.5705\n",
      "Epoch 5/50\n",
      "567/567 [==============================] - 3s 5ms/sample - loss: 6.5866 - accuracy: 0.5705\n",
      "Epoch 6/50\n",
      "567/567 [==============================] - 3s 5ms/sample - loss: 6.5866 - accuracy: 0.5705\n",
      "Epoch 7/50\n",
      "567/567 [==============================] - 3s 5ms/sample - loss: 6.5866 - accuracy: 0.5705\n",
      "Epoch 8/50\n",
      "567/567 [==============================] - 3s 5ms/sample - loss: 6.5866 - accuracy: 0.5705\n",
      "Epoch 9/50\n",
      "567/567 [==============================] - 3s 5ms/sample - loss: 6.5866 - accuracy: 0.5705\n",
      "Epoch 10/50\n",
      " 64/567 [==>...........................] - ETA: 2s - loss: 6.7100 - accuracy: 0.5625"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e8ca09b4c37f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m                   metrics=['accuracy'])\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/codev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/codev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/codev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/codev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/codev/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/codev/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/codev/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/codev/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/codev/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/codev/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/codev/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "featuresfiles = ['VGG19.features.npy', 'VGG16.features.npy']\n",
    "\n",
    "def make_cnn():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=[7, 7, 512]))\n",
    "    model.add(layers.Dense(2000))\n",
    "    model.add(layers.LeakyReLU())\n",
    "#     model.add(layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same',\n",
    "#                                      input_shape=[7, 7, 512]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "#     model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(500))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Dense(100))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Dense(4))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "for features_file in featuresfiles:\n",
    "    \n",
    "    # Loading features\n",
    "    features = np.load(features_path+features_file)\n",
    "    print('\\nModel:', features_file.split('.')[0])\n",
    "    \n",
    "    # Loading y\n",
    "    y = np.load(features_path+'y.lab.npy')\n",
    "    \n",
    "    # Flattening\n",
    "    print('Features shape:', features.shape)\n",
    "\n",
    "    # Séparation en test et entraînement 30/70\n",
    "    split_indice = int(0.7 * len(features))\n",
    "    x_train = features[:split_indice - 1]\n",
    "    y_train = y[:split_indice - 1]\n",
    "    x_test = features[split_indice:]\n",
    "    y_test = y[split_indice:]\n",
    "\n",
    "    print_distribution(y_train)\n",
    "    print_distribution(y_test)\n",
    "    del features, y\n",
    "    \n",
    "    model = make_cnn()\n",
    "\n",
    "\n",
    "    model.compile(optimizer='sgd',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=50)\n",
    "\n",
    "    model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "#     print('MLP train accuracy:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
